#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Mon Oct 30 16:40:15 2017

@author: eric.hensleyibm.com
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.model_selection import learning_curve
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score
from sklearn.decomposition import PCA
import lightgbm as lgm


np.random.seed(42)
data = pd.read_csv('ncaadata.csv')
features = list(data)[1:]
x_feat = features[:-1]
#features = list(data)
#x_feat = features[:-1]
x = data[x_feat]
y = data['y']

scores = ['accuracy', 'roc_auc', 'f1_weighted', 'neg_log_loss']
pipe = Pipeline([
    ('preprocess', StandardScaler()),
    ('reduce_dim', PCA(random_state = 1108, n_components=16,whiten=True,svd_solver='full')),
    ('classify', GaussianNB())
])
N_FEATURES_OPTIONS = range(2, 40)
param_grid = [
    {
        'reduce_dim__n_components': N_FEATURES_OPTIONS, 
    }
]

grid = GridSearchCV(pipe, cv=10, n_jobs=1, param_grid=param_grid, refit = False, scoring = scores)
grid.fit(x, y.astype(int))


scores = ['accuracy', 'roc_auc', 'f1_weighted', 'neg_log_loss']
pipe = Pipeline([
    ('normalize', StandardScaler()),
    ('classify', ExtraTreesClassifier(random_state = 1108, n_estimators = 30, min_samples_leaf = 10, n_jobs = -1, max_depth = 12))
])

 
qwerty = [12,30, 40]
param_grid = [
    {
        'classify__max_depth': qwerty
    }
]


grid = GridSearchCV(pipe, cv=25, n_jobs=1, param_grid=param_grid, refit = False, scoring = scores)
grid.fit(x, y.astype(int))

results = grid.cv_results_
plt.figure(figsize=(10, 10))
plt.title("GridSearchCV evaluating using multiple scorers simultaneously",
          fontsize=16)

plt.xlabel("n features")
plt.ylabel("Score")
plt.grid()

ax = plt.axes()
ax.set_xlim(2, 60)
ax.set_ylim(.45, .8)

# Get the regular numpy array from the MaskedArray
X_axis = np.array(results['param_reduce_dim__n_components'].data, dtype=float)

for scorer, color in zip(sorted(scores), ['g', 'k', 'r', 'y']):
    if scorer != 'neg_log_loss':
        for sample, style in (('train', '--'), ('test', '-')):
            sample_score_mean = results['mean_%s_%s' % (sample, scorer)]
            sample_score_std = results['std_%s_%s' % (sample, scorer)]
            ax.fill_between(X_axis, sample_score_mean - sample_score_std,
                            sample_score_mean + sample_score_std,
                            alpha=0.1 if sample == 'test' else 0, color=color)
            ax.plot(X_axis, sample_score_mean, style, color=color,
                    alpha=1 if sample == 'test' else 0.7,
                    label="%s (%s)" % (scorer, sample))
        best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]
        best_score = results['mean_test_%s' % scorer][best_index]
    
        # Plot a dotted vertical line at the best score for that scorer marked by x
        ax.plot([X_axis[best_index], ] * 2, [0, best_score],
                linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)
    
        # Annotate the best score for that scorer
        ax.annotate("%0.2f" % best_score,
                    (X_axis[best_index], best_score + 0.005))
    
    elif scorer == 'neg_log_loss':
        for sample, style in (('train', '--'), ('test', '-')):
            samplemean = results['mean_%s_%s' % (sample, scorer)]
            sample_score_mean = []
            for each in samplemean:
                sample_score_mean.append(each * -1)
            sample_score_mean = np.array(sample_score_mean)
            samplestd = results['std_%s_%s' % (sample, scorer)]
            sample_score_std = []
            for each in samplestd:
                sample_score_std.append(each * -1)
            sample_score_std = np.array(sample_score_std)
            ax.fill_between(X_axis, sample_score_mean - sample_score_std,
                            sample_score_mean + sample_score_std,
                            alpha=0.1 if sample == 'test' else 0, color=color)
            ax.plot(X_axis, sample_score_mean, style, color=color,
                    alpha=1 if sample == 'test' else 0.7,
                    label="%s (%s)" % (scorer, sample))            

        best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]
        best_score = results['mean_test_%s' % scorer][best_index]
    
        # Plot a dotted vertical line at the best score for that scorer marked by x
        ax.plot([X_axis[best_index], ] * 2, [0, best_score*-1],
                linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)
    
        # Annotate the best score for that scorer
        ax.annotate("%0.2f" % best_score,
                    (X_axis[best_index], best_score + 0.005))

plt.legend(loc="best")
plt.grid('off')
plt.show()




results = grid.cv_results_
plt.figure(figsize=(10, 10))
plt.title("GridSearchCV evaluating using multiple scorers simultaneously",
          fontsize=16)

plt.xlabel("n features")
plt.ylabel("Score")
plt.grid()

ax = plt.axes()
ax.set_xlim(10, 40)
ax.set_ylim(.45, .55)

# Get the regular numpy array from the MaskedArray
X_axis = np.array(results['param_classify__max_depth'].data, dtype=float)

for scorer, color in zip(sorted(scores), ['g', 'k', 'r', 'y']):
    if scorer != 'neg_log_loss':
        for sample, style in (('train', '--'), ('test', '-')):
            sample_score_mean = results['mean_%s_%s' % (sample, scorer)]
            sample_score_std = results['std_%s_%s' % (sample, scorer)]
            ax.fill_between(X_axis, sample_score_mean - sample_score_std,
                            sample_score_mean + sample_score_std,
                            alpha=0.1 if sample == 'test' else 0, color=color)
            ax.plot(X_axis, sample_score_mean, style, color=color,
                    alpha=1 if sample == 'test' else 0.7,
                    label="%s (%s)" % (scorer, sample))
        best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]
        best_score = results['mean_test_%s' % scorer][best_index]
    
        # Plot a dotted vertical line at the best score for that scorer marked by x
        ax.plot([X_axis[best_index], ] * 2, [0, best_score],
                linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)
    
        # Annotate the best score for that scorer
        ax.annotate("%0.2f" % best_score,
                    (X_axis[best_index], best_score + 0.005))
    
    elif scorer == 'neg_log_loss':
        for sample, style in (('train', '--'), ('test', '-')):
            samplemean = results['mean_%s_%s' % (sample, scorer)]
            sample_score_mean = []
            for each in samplemean:
                sample_score_mean.append(each * -1)
            sample_score_mean = np.array(sample_score_mean)
            samplestd = results['std_%s_%s' % (sample, scorer)]
            sample_score_std = []
            for each in samplestd:
                sample_score_std.append(each * -1)
            sample_score_std = np.array(sample_score_std)
            ax.fill_between(X_axis, sample_score_mean - sample_score_std,
                            sample_score_mean + sample_score_std,
                            alpha=0.1 if sample == 'test' else 0, color=color)
            ax.plot(X_axis, sample_score_mean, style, color=color,
                    alpha=1 if sample == 'test' else 0.7,
                    label="%s (%s)" % (scorer, sample))            

        best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]
        best_score = results['mean_test_%s' % scorer][best_index]
    
        # Plot a dotted vertical line at the best score for that scorer marked by x
        ax.plot([X_axis[best_index], ] * 2, [0, best_score*-1],
                linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)
    
        # Annotate the best score for that scorer
        ax.annotate("%0.2f" % best_score,
                    (X_axis[best_index], best_score + 0.005))

plt.legend(loc="best")
plt.grid('off')
plt.show()




accuracy = []
rocauc = []
f1 = []
rocscores = []
accscores = []
f1scores = []
rocscores1 = []
accscores1 = []
f1scores1 = []
for j in range(1, 30):
    params = None
    trainx, testx, trainy, testy = train_test_split(data[x_feat], y, random_state = j, test_size = .08)
    trainx = StandardScaler().fit_transform(trainx)
    testx = StandardScaler().fit_transform(testx)
    trainx1 = PCA(n_components = 18, random_state = 1108).fit_transform(trainx)
    trainx2 = PCA(n_components = 18, random_state = 1108).fit_transform(trainx)
    testx1 = PCA(n_components = 18, random_state = 1108).fit_transform(testx)
    testx2 = PCA(n_components = 18, random_state = 1108).fit_transform(testx)
    if trainx1.any() != trainx2.any():
        print 'break'        
    clf = GaussianNB()
    clf.fit(trainx, trainy)
    accscores.append(accuracy_score(testy, clf.predict(testx)))
    rocscores.append(roc_auc_score(testy, clf.predict(testx)))
    f1scores.append(f1_score(testy, clf.predict(testx)))
    clf = GaussianNB()
    clf.fit(trainx1, trainy)
    accscores1.append(accuracy_score(testy, clf.predict(testx1)))
    rocscores1.append(roc_auc_score(testy, clf.predict(testx1)))
    f1scores1.append(f1_score(testy, clf.predict(testx1)))
print sum(accscores)/len(accscores)
print sum(rocscores)/len(rocscores)
print sum(f1scores)/len(f1scores)
print sum(accscores1)/len(accscores1)
print sum(rocscores1)/len(rocscores1)
print sum(f1scores1)/len(f1scores1)

accuracy.append(sum(accscores)/len(accscores))
rocauc.append(sum(rocscores)/len(accscores))
f1.append(sum(f1scores)/len(f1scores))
for v in range(1, 10):
    dropset = []
    keepset = []
    for i in range(1, 31):
        trainx, testx, trainy, testy = train_test_split(x, y, random_state = i, test_size = .2)
        trainx = StandardScaler().fit_transform(trainx)
        trainx1 = PCA(n_components = 18, random_state = 1108).fit_transform(trainx)
        trainx2 = PCA(n_components = 18, random_state = 1108).fit_transform(trainx)
        if trainx1.any() != trainx2.any():
            print 'break'
        
        testx = StandardScaler().fit_transform(testx)
        model = ExtraTreesClassifier(random_state = 1108, n_estimators = 30, min_samples_leaf = 10, n_jobs = -1, max_depth = 12)
        model.fit(trainx, trainy)
        params = model.feature_importances_
        multiplier = float(v)/100000
        thresh = ((max(params)-min(params))*(multiplier))+min(params)
        for each in range(0, len(params)):
            if params[each] >= thresh:
                pass
            elif params[each] < thresh and x_feat[each] not in dropset:
                dropset.append(x_feat[each])
    for every in x_feat:
        if every not in dropset:
            keepset.append(every)
    rocscores = []
    accscores = []
    f1scores = []
    print len(keepset)
    for j in range(101, 120):
        params = None
        trainx, testx, trainy, testy = train_test_split(data[keepset], y, random_state = j, test_size = .08)
        trainx = StandardScaler().fit_transform(trainx)
        testx = StandardScaler().fit_transform(testx)
        clf = GaussianNB()
        clf.fit(trainx, trainy)
        accscores.append(accuracy_score(testy, clf.predict(testx)))
        rocscores.append(roc_auc_score(testy, clf.predict(testx)))
        f1scores.append(f1_score(testy, clf.predict(testx)))
    print sum(accscores)/len(accscores)
    print sum(rocscores)/len(rocscores)
    print sum(f1scores)/len(f1scores)
    accuracy.append(sum(accscores)/len(accscores))
    rocauc.append(sum(rocscores)/len(accscores))
    f1.append(sum(f1scores)/len(f1scores))
plt.figure()
plt.xlabel("Training examples")
plt.ylabel("Score")
plt.axes().set_ylim(.49, .54)
train_sizes = range(0,len(accuracy))
plt.grid()
plt.plot(train_sizes, accuracy, 'o-', color="r",
         label="Accuracy")
plt.plot(train_sizes, f1, 'o-', color="g",
         label="F1")
plt.plot(train_sizes, rocauc, 'o-', color="b",
         label="AUC")
plt.legend(loc="best")
